{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# üöÄ Aula Aberta de DQN\n",
    "> Ensinando um m√≥dulo a pousar na Lua com redes neurais!\n",
    "\n",
    "> Voc√™ pode checar os slides da aula [aqui](#todo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importando as bibliotecas necess√°rias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch # Necess√°rio para criar redes neurais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Box2D # Necess√°rio para nossos ambientes\n",
    "!pip install gym[box2d] # Ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "## Replay Buffer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer para DQNs.\"\"\"\n",
    "    def __init__(self, max_length, observation_space):\n",
    "        \"\"\"Cria um Replay Buffer.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        max_length: int\n",
    "            Tamanho m√°ximo do Replay Buffer.\n",
    "        observation_space: int\n",
    "            Tamanho do espa√ßo de observa√ß√£o.\n",
    "        \"\"\"\n",
    "        # Crie os atributos self.index, self.size e os atribua o valor 0\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        # Crie o atributo self.max_length que recebe o valor do par√¢metro \n",
    "        ...\n",
    "\n",
    "        # Utilizando a fun√ß√£o np.zeros inicialize a mem√≥ria para cada vari√°vel com o formato indicado:\n",
    "        # self.states - formato(max_length, observation_space), array de np.float32\n",
    "        ...\n",
    "        # self.actions - formato(max_length), array de np.int32\n",
    "        ...\n",
    "        # self.rewards - formato(max_lenght), array de np.float32\n",
    "        ...\n",
    "        # self.next_states - formato(max_lenght, observation_space), array de np.float32\n",
    "        ...\n",
    "        # self.dones - formato(max_length), array de np.int32\n",
    "        ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
    "        # Retorna o atributo self.size\n",
    "        return ...\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adiciona uma experi√™ncia ao Replay Buffer.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        state: np.array\n",
    "            Estado da transi√ß√£o.\n",
    "        action: int\n",
    "            A√ß√£o tomada.\n",
    "        reward: float\n",
    "            Recompensa recebida.\n",
    "        state: np.array\n",
    "            Estado seguinte.\n",
    "        done: int\n",
    "            Flag indicando se o epis√≥dio acabou.\n",
    "        \"\"\"\n",
    "\n",
    "        # Para cada array de cada par√¢metro, adicione o par√¢metro ao array no √≠ndice self.index\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        # Incrementa o √≠ndice e atualiza o tamanho\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "        if self.size < self.max_length:\n",
    "            self.size += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Retorna um batch de experi√™ncias.\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            Tamanho do batch de experi√™ncias.\n",
    "        Retorna\n",
    "        -------\n",
    "        states: np.array\n",
    "            Batch de estados.\n",
    "        actions: np.array\n",
    "            Batch de a√ß√µes.\n",
    "        rewards: np.array\n",
    "            Batch de recompensas.\n",
    "        next_states: np.array\n",
    "            Batch de estados seguintes.\n",
    "        dones: np.array\n",
    "            Batch de flags indicando se o epis√≥dio acabou.\n",
    "        \"\"\"\n",
    "\n",
    "        # Utilizando a fun√ß√£o np.random.randint(), atribua a vari√°vel idxs \n",
    "        # um array de tamanho batch_size, com n√∫meros aleat√≥rios entre 0 e self.size\n",
    "        idxs = ...\n",
    "\n",
    "        # Para cada elemento da observa√ß√£o, retorne um batch de elementos que est√£o nos\n",
    "        # √≠ndices idxs de cada array de mem√≥ria\n",
    "        return(...)"
   ]
  },
  {
   "source": [
    "## Conhecendo nosso ambiente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "print(f\"Formato das observa√ß√µes do nosso agente: {env.observation_space.shape} | Uma poss√≠vel observa√ß√£o: {env.observation_space.sample()}\")\n",
    "print(f\"N√∫mero de poss√≠veis a√ß√µes: {env.action_space.n} | Uma poss√≠vel a√ß√£o: {env.action_space.sample()}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "source": [
    "Como est√£o configuradas as recompensas:\n",
    "  - Se a nave pousar ela recebe uma recompensa de $+100$\n",
    "  - Cada perna que entra em contato com o solo o agente recebe $+10$\n",
    "  - Acionar as engines faz com que ele receba uma penalidade de $-0.3$ por frame\n",
    "  - O estado terminal ocorre quando ou o agente morre, recebendo uma penalidade de $-100$ ou quando acaba o tempo da simula√ß√£o, recebendo mais uma recompensa de $+100$ se estava pousado no alvo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Criando um agente aleat√≥rio\n",
    "S√≥ para ver se est√° tudo funcionando corretamente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_env():\n",
    "\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_env()"
   ]
  },
  {
   "source": [
    "## Rede Neural"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cria uma rede neural para DQN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Inicializa a rede\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        in_dim: int\n",
    "        Dimens√£o de entrada da rede, ou seja, o shape do estado do ambiente\n",
    "        \n",
    "        out_dim: int\n",
    "        N√∫mero de a√ß√µes do agente neste ambiente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LinearNetwork, self).__init__()\n",
    "\n",
    "        # Cire um atributo self.layers com nossa rede neural utilizando a fun√ß√£o nn.Sequential\n",
    "        # A estrutura da rede deve ser:\n",
    "        # Linear(in_dim, 128) -> ReLU() -> Linear(128,128) -> ReLU() -> Linear(128, out_dim)\n",
    "\n",
    "        self.layers = ...\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como mem√≥ria\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=3e-4, \n",
    "                 gamma=0.9, \n",
    "                 max_memory=10000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01,\n",
    "                 epochs=1):\n",
    "      \n",
    "        \"\"\"\n",
    "        Inicializa o agente com os par√¢metros dados\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        \n",
    "        observation_space: gym.spaces\n",
    "        O espa√ßo de observa√ß√£o do gym\n",
    "         \n",
    "        action_space: gym.spaces\n",
    "        O espa√ßo de a√ß√µes do agente modelado no gym\n",
    "        \n",
    "        lr: floar, default=3e-4\n",
    "        A taxa de aprendizado do agente\n",
    "        \n",
    "        gamma: float, default=0.99\n",
    "        O fator de desconto. Se perto de 1. as recompensas futuras ter√£o grande import√¢ncia,\n",
    "        se perto de 0. as recompensas mais instant√¢neas ter√£o maior import√¢ncia\n",
    "        \n",
    "        max_memory: int, default=100000\n",
    "        O n√∫mero m√°ximo de transi√ß√µes armazenadas no buffer de mem√≥ria\n",
    "        \n",
    "        epsilon_init: float, default=0.5\n",
    "        O epsilon inicial do agente. Se pr√≥ximo de 1. o agente tomar√° muitas a√ß√µes\n",
    "        aleat√≥rias, se pro√≥ximo de 0. o agente escolher√° as a√ß√µes com maior\n",
    "        Q-valor\n",
    "        \n",
    "        epsilon_decay: float, default=0.9995\n",
    "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
    "        a escolher meno a√ß√µes aleat√≥rias se epsilon_decay<1\n",
    "        \n",
    "        min_epsilon: float, default=0.01\n",
    "        O menor epsilon poss√≠vel\n",
    "        \n",
    "        \n",
    "        network: str, default='linear'\n",
    "        O tipo de rede a ser utilizada para o agente DQN. Por padr√£o √© usada uma rede linear, mas\n",
    "        pode ser usada uma rede convolucional se o par√¢metro for 'conv'\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Inicialize o atributo de self.device com o m√©todo torch.device, se poss√≠vel utlize cuda,\n",
    "        # caso contr√°rio utilize uma cpu\n",
    "        ...\n",
    "        # Inicialize o atributo self.gamma\n",
    "        ...\n",
    "        # Inicialize o atributo self.memory inicializando um objeto ReplayBuffer()\n",
    "        ...\n",
    "        # Inicialize o atributo self.action_space\n",
    "        ...\n",
    "        # Inicialize o atributo self.epoch\n",
    "        ...\n",
    "\n",
    "        # Inicialize os atributo self.epsilon, self.epsilon_decay e self.espilon_min\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        # Inicialize o atributo de self.dqn com nosso objeto LinearNetwork(),\n",
    "        # utilize tamb√©m o m√©todo .to(self.device)\n",
    "        ...\n",
    "\n",
    "        # Inicialize o atributo self.optimizer com o optimizador optim.Adam() que optimiza o\n",
    "        # self.dqn.parameters() com um learning rate lr\n",
    "        ...\n",
    "\n",
    "    def act(self, state, greedy=False):\n",
    "        \"\"\"\n",
    "        M√©todo para o agente escolher uma a√ß√£o\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        \n",
    "        state\n",
    "        O estado do agente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        \n",
    "        action\n",
    "        A a√ß√£o escolhida pelo agente\n",
    "        \"\"\"\n",
    "\n",
    "        # Utilize do algoritmo epsilon-greedy:\n",
    "        # se um n√∫mero criado pela fun√ß√£o np.random.random() for menor\n",
    "        # que nosso epsilon e greedy for Falso, execute uma a√ß√£o aleat√≥ria\n",
    "        if ... :\n",
    "            ...\n",
    "        else:\n",
    "            with torch.no_grad(): # Utilizamos no_grad j√° que n√£o iremos optimzar esses par√¢metros agora\n",
    "                # Transforme o par√¢metro state em um torch.FloatTensor(), utilize o m√©todo .to(self.device)\n",
    "                ...\n",
    "                # Passe o state para nossa DQN pelo seu m√©todo forward e colete a a√ß√£o de maior valor com\n",
    "                # o m√©todo .argmax(dim=-1)\n",
    "                ...\n",
    "                # Transforme a a√ß√£o em um valor numpy com os m√©todos a√ß√£o.cpu().numpy()\n",
    "                ...\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def eps_decay(self):\n",
    "        \"\"\"\n",
    "        M√©todo para aplicar o decaimento do epsilon\n",
    "        \"\"\"\n",
    "\n",
    "        # Se self.epsilon for maior que o self.epsilon_min,\n",
    "        # self.epsilon = self.epsilon * self.epsilon_decay\n",
    "        ...\n",
    "        ...\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        M√©todo para armazenar uma sequ√™ncia estado, a√ß√£o, recompensa, pr√≥ximo estado e done\n",
    "        no buffer de mem√≥ria\n",
    "        \"\"\"\n",
    "\n",
    "        # utilizando o m√©todo .update do nosso ReplaBuffer, passe os par√¢metros √† nossa mem√≥ria\n",
    "        ...\n",
    "    \n",
    "    def train(self, batch_size, save_file = False):\n",
    "        \"\"\" \n",
    "        M√©todo para treinar o agente\n",
    "        \"\"\"\n",
    "\n",
    "        # se batch_size * 10 for menor que o tamanho de nossa mem√≥ria, a a gente n√£o deve treinar\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Colete um batch de experi√™ncias com o m√©todo .sample da nossa mem√≥ria\n",
    "            ... \n",
    "\n",
    "            # Transforme cada atributo da nossa mem√≥ria em um tensor, para as actions,\n",
    "            # rewards, e dones utilize o m√©todo .unsqueeze(-1) para coloc√°-los no formato certo\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "\n",
    "            # Para obter nossos Q valores passe os states para nossa DQN pelo m√©todo .forward(),\n",
    "            # Utilize tamb√©m o m√©todo .gather(-1, actions.long()) ap√≥s o forward para obter os q valores\n",
    "            ...\n",
    "\n",
    "            with torch.no_grad():  # Utilizamos o no_grad pois esse q vamos usar para a loss, n√£o precisa dos gradientes\n",
    "                # Obtenha o Q2 passando os next_states para nossa DQN pelo m√©todo .forward(),\n",
    "                # utilize depois o m√©todo .max(dim=-1, keepdim=True)[0]\n",
    "                ...\n",
    "\n",
    "                # Calcule o target com a f√≥rmula (rewards + (1 - dones) * self.gamma * q2)\n",
    "                ...\n",
    "\n",
    "            # Calcule a loss\n",
    "            ...\n",
    "            # Realize as etapas de optimiza√ß√£o\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "\n",
    "        if save:\n",
    "            self.save_model()\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        torch.save(self.dqn.state_dict(), model_file)\n",
    "        print(f\"\\n Model saved: {model_file}\")\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        self.dqn.load_state_dict(torch.load(model_file))\n",
    "        print(f\"Model loaded: {model_file}\")"
   ]
  },
  {
   "source": [
    "### O que o m√©todo unsqueeze() faz?\n",
    "Basicamente ele insere uma dimens√£o de valor um em alguma dimens√£o especificada de um tensor, exemplos:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor sem unsqueeze: \ntensor([[1, 2, 3],\n        [4, 5, 6]])\nFormato do tensor sem unsqueeze torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.as_tensor([[1,2,3],[4,5,6]])\n",
    "print(f\"Tensor sem unsqueeze: \\n{a}\\n\"\n",
    "      f\"Formato do tensor sem unsqueeze {a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor com unsqueeze: \ntensor([[[1],\n         [2],\n         [3]],\n\n        [[4],\n         [5],\n         [6]]])\nFormato do tensor com unsqueeze torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "b = a.unsqueeze(-1)\n",
    "print(f\"Tensor com unsqueeze: \\n{b}\\n\"\n",
    "      f\"Formato do tensor com unsqueeze {b.shape}\")"
   ]
  },
  {
   "source": [
    "### O que o m√©todo gather() faz?\n",
    "Basicamente ele criar um tensor novo pegando os valores de um tensor (no nosso caso actions.long()) e os utilizando como √≠ndices para pegar valores do tensor de origem (sa√≠da de nossa rede neural). o par√¢metro dim √© utilziado para indicar por qual dimens√£o do tensor de √≠ndice deve ser percorrido.\n",
    "\n",
    "![img](https://i.stack.imgur.com/nudGq.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Treinamento"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, timesteps, batch_size, render=False):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "    episode = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for timestep in range(1, timesteps + 1):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Tomar a a√ß√£o escolhida\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Guardar as informa√ß√µes geradas pela a√ß√£o\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Treinar a rede com base no ReplayBuffer\n",
    "        agent.train(batch_size, False)\n",
    "\n",
    "        # Soma as recompensas\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        agent.eps_decay()\n",
    "\n",
    "        if episode_returns:\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        ratio = math.ceil(100 * timestep / timesteps)\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "\n",
    "        # Atualiza o estado\n",
    "        state = next_state\n",
    "\n",
    "        if render:\n",
    "            # Mostra o ambiente\n",
    "            env.render()\n",
    "\n",
    "        print(\n",
    "            f\"\\r[{ratio:3d}%]\",\n",
    "            f\"timestep = {timestep}/{timesteps}\",\n",
    "            f\"episode = {episode:3d}\",\n",
    "            f\"avg_return = {avg_return:10.4f}\",\n",
    "            f\"eps = {agent.epsilon:.4f}\",\n",
    "            sep=\", \",\n",
    "            end=\"\")\n",
    "\n",
    "    env.close()\n",
    "    return avg_returns"
   ]
  },
  {
   "source": [
    "## Treinando o agente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 1\n",
    "EPS_END = 0.001\n",
    "EPS_DECAY = 0.99995\n",
    "MAX_MEMORY = 1_000_000\n",
    "TIMESTEPS = 150_000\n",
    "EPOCHS = 1\n",
    "\n",
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "\n",
    "print(\"\\nTraining DQN\")\n",
    "dqn_net = DQNagent(observation_space=OBS_SPACE,\n",
    "                    action_space=ACT_SPACE,\n",
    "                    lr=3e-4,\n",
    "                    gamma=GAMMA,\n",
    "                    max_memory=MAX_MEMORY,\n",
    "                    epsilon_init=EPS_INIT,\n",
    "                    epsilon_decay=EPS_DECAY,\n",
    "                    epsilon_min=EPS_END,\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "\n",
    "results_dqn = train(dqn_net, env, TIMESTEPS, BATCH_SIZE, render=False)"
   ]
  },
  {
   "source": [
    "## Checando os resultados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plota os resultados\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(results_dqn)\n",
    "plt.title('DQN - m√©dia m√≥vel do retorno nos √∫timos 20 epis√≥dios')\n",
    "plt.xlabel('timestep')\n",
    "plt.ylabel('retorno')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env, episodes):\n",
    "    for episode in range(episodes):\n",
    "        done = 0\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state, greedy=True)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            total_reward += reward\n",
    "\n",
    "            print(f\"\\r {total_reward:3.3f}\", end=\"\")\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "test(dqn_net, env, 5)"
   ]
  }
 ]
}