{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Lunar-Lander-DQN-Aula.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1_sx6V9omrl"
      },
      "source": [
        "# üöÄ Aula Aberta de DQN\n",
        "> Ensinando um m√≥dulo a pousar na Lua com redes neurais!\n",
        ">\n",
        "> Voc√™ pode checar os slides da aula [aqui](../Deep%20Q-Networks%20(DQN).pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFIIz0l4omr8"
      },
      "source": [
        "## Importando as bibliotecas necess√°rias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWnrsCwyomr9"
      },
      "source": [
        "# Necess√°rio no colab\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install pyvirtualdisplay PyOpenGL PyOpenGL-accelerate\n",
        "import pyvirtualdisplay\n",
        "pyvirtualdisplay.Display(visible=False, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qapmkT-_omr_"
      },
      "source": [
        "!pip install torch # Necess√°rio para criar redes neurais\n",
        "!pip install gym[box2d] # Necess√°rio para nossos ambientes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvsF5YlnomsA"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4hko43JomsA"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFC4VyYRomsB"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjn9Sp-BomsB"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Experience Replay Buffer para DQNs.\"\"\"\n",
        "    def __init__(self, max_length, observation_space):\n",
        "        \"\"\"Cria um Replay Buffer.\n",
        "\n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        max_length: int\n",
        "            Tamanho m√°ximo do Replay Buffer.\n",
        "        observation_space: int\n",
        "            Tamanho do espa√ßo de observa√ß√£o.\n",
        "        \"\"\"\n",
        "        # Crie os atributos self.index, self.size e os atribua o valor 0\n",
        "        ...\n",
        "        ...\n",
        "\n",
        "        # Crie o atributo self.max_length que recebe o valor do par√¢metro \n",
        "        ...\n",
        "\n",
        "        # Utilizando a fun√ß√£o np.zeros inicialize a mem√≥ria para cada vari√°vel com o formato indicado:\n",
        "        # self.states - formato(max_length, observation_space), array de np.float32\n",
        "        ...\n",
        "        # self.actions - formato(max_length), array de np.int32\n",
        "        ...\n",
        "        # self.rewards - formato(max_lenght), array de np.float32\n",
        "        ...\n",
        "        # self.next_states - formato(max_lenght, observation_space), array de np.float32\n",
        "        ...\n",
        "        # self.dones - formato(max_length), array de np.int32\n",
        "        ...\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"Retorna o tamanho do buffer.\"\"\"\n",
        "        # Retorna o atributo self.size\n",
        "        return ...\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Adiciona uma experi√™ncia ao Replay Buffer.\n",
        "\n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        state: np.array\n",
        "            Estado da transi√ß√£o.\n",
        "        action: int\n",
        "            A√ß√£o tomada.\n",
        "        reward: float\n",
        "            Recompensa recebida.\n",
        "        state: np.array\n",
        "            Estado seguinte.\n",
        "        done: int\n",
        "            Flag indicando se o epis√≥dio acabou.\n",
        "        \"\"\"\n",
        "\n",
        "        # Para cada array de cada par√¢metro, adicione o par√¢metro ao array no √≠ndice self.index\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "\n",
        "        # Incrementa o √≠ndice e atualiza o tamanho\n",
        "        self.index = (self.index + 1) % self.max_length\n",
        "        if self.size < self.max_length:\n",
        "            self.size += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Retorna um batch de experi√™ncias.\n",
        "        \n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            Tamanho do batch de experi√™ncias.\n",
        "        Retorna\n",
        "        -------\n",
        "        states: np.array\n",
        "            Batch de estados.\n",
        "        actions: np.array\n",
        "            Batch de a√ß√µes.\n",
        "        rewards: np.array\n",
        "            Batch de recompensas.\n",
        "        next_states: np.array\n",
        "            Batch de estados seguintes.\n",
        "        dones: np.array\n",
        "            Batch de flags indicando se o epis√≥dio acabou.\n",
        "        \"\"\"\n",
        "\n",
        "        # Utilizando a fun√ß√£o np.random.randint(), atribua a vari√°vel idxs \n",
        "        # um array de tamanho batch_size, com n√∫meros aleat√≥rios entre 0 e self.size\n",
        "        idxs = ...\n",
        "\n",
        "        # Para cada elemento da observa√ß√£o, retorne um batch de elementos que est√£o nos\n",
        "        # √≠ndices idxs de cada array de mem√≥ria\n",
        "        return(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtbrk6bromsD"
      },
      "source": [
        "## Conhecendo nosso ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viGyLs-YomsD"
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "print(f\"Formato das observa√ß√µes do nosso agente: {env.observation_space.shape} | Uma poss√≠vel observa√ß√£o: {env.observation_space.sample()}\")\n",
        "print(f\"N√∫mero de poss√≠veis a√ß√µes: {env.action_space.n} | Uma poss√≠vel a√ß√£o: {env.action_space.sample()}\")\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nddTjeEaomsE"
      },
      "source": [
        "Como est√£o configuradas as recompensas:\n",
        "  - Se a nave pousar ela recebe uma recompensa de $+100$\n",
        "  - Cada perna que entra em contato com o solo o agente recebe $+10$\n",
        "  - Acionar as engines faz com que ele receba uma penalidade de $-0.3$ por frame\n",
        "  - O estado terminal ocorre quando ou o agente morre, recebendo uma penalidade de $-100$ ou quando acaba o tempo da simula√ß√£o, recebendo mais uma recompensa de $+100$ se estava pousado no alvo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJLHPexAomsF"
      },
      "source": [
        "### Criando um agente aleat√≥rio\n",
        "S√≥ para ver se est√° tudo funcionando corretamente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpesQ1DcomsF"
      },
      "source": [
        "def random_env():\n",
        "\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        env.render()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    env.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgaIKtlIomsG"
      },
      "source": [
        "random_env()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12OH_FjfomsG"
      },
      "source": [
        "## Rede Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufwmshHlomsG"
      },
      "source": [
        "class LinearNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Cria uma rede neural para DQN\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        \"\"\"\n",
        "        Inicializa a rede\n",
        "        \n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        in_dim: int\n",
        "        Dimens√£o de entrada da rede, ou seja, o shape do estado do ambiente\n",
        "        \n",
        "        out_dim: int\n",
        "        N√∫mero de a√ß√µes do agente neste ambiente\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LinearNetwork, self).__init__()\n",
        "\n",
        "        # Cire um atributo self.layers com nossa rede neural utilizando a fun√ß√£o nn.Sequential\n",
        "        # A estrutura da rede deve ser:\n",
        "        # Linear(in_dim, 128) -> ReLU() -> Linear(128,128) -> ReLU() -> Linear(128, out_dim)\n",
        "\n",
        "        self.layers = ...\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dciPPaxDomsH"
      },
      "source": [
        "class DQNagent:\n",
        "    \"\"\"\n",
        "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como mem√≥ria\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 observation_space, \n",
        "                 action_space, \n",
        "                 lr=3e-4, \n",
        "                 gamma=0.9, \n",
        "                 max_memory=10000,\n",
        "                 epsilon_init=0.5,\n",
        "                 epsilon_decay=0.995,\n",
        "                 epsilon_min=0.01,\n",
        "                 epochs=1):\n",
        "      \n",
        "        \"\"\"\n",
        "        Inicializa o agente com os par√¢metros dados\n",
        "        \n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        \n",
        "        observation_space: gym.spaces\n",
        "        O espa√ßo de observa√ß√£o do gym\n",
        "         \n",
        "        action_space: gym.spaces\n",
        "        O espa√ßo de a√ß√µes do agente modelado no gym\n",
        "        \n",
        "        lr: floar, default=3e-4\n",
        "        A taxa de aprendizado do agente\n",
        "        \n",
        "        gamma: float, default=0.99\n",
        "        O fator de desconto. Se perto de 1. as recompensas futuras ter√£o grande import√¢ncia,\n",
        "        se perto de 0. as recompensas mais instant√¢neas ter√£o maior import√¢ncia\n",
        "        \n",
        "        max_memory: int, default=100000\n",
        "        O n√∫mero m√°ximo de transi√ß√µes armazenadas no buffer de mem√≥ria\n",
        "        \n",
        "        epsilon_init: float, default=0.5\n",
        "        O epsilon inicial do agente. Se pr√≥ximo de 1. o agente tomar√° muitas a√ß√µes\n",
        "        aleat√≥rias, se pro√≥ximo de 0. o agente escolher√° as a√ß√µes com maior\n",
        "        Q-valor\n",
        "        \n",
        "        epsilon_decay: float, default=0.9995\n",
        "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
        "        a escolher meno a√ß√µes aleat√≥rias se epsilon_decay<1\n",
        "        \n",
        "        min_epsilon: float, default=0.01\n",
        "        O menor epsilon poss√≠vel\n",
        "        \n",
        "        \n",
        "        network: str, default='linear'\n",
        "        O tipo de rede a ser utilizada para o agente DQN. Por padr√£o √© usada uma rede linear, mas\n",
        "        pode ser usada uma rede convolucional se o par√¢metro for 'conv'\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        # Inicialize o atributo de self.device com o m√©todo torch.device, se poss√≠vel utlize cuda,\n",
        "        # caso contr√°rio utilize uma cpu\n",
        "        ...\n",
        "        # Inicialize o atributo self.gamma\n",
        "        ...\n",
        "        # Inicialize o atributo self.memory inicializando um objeto ReplayBuffer()\n",
        "        ...\n",
        "        # Inicialize o atributo self.action_space\n",
        "        ...\n",
        "        # Inicialize o atributo self.epoch\n",
        "        ...\n",
        "\n",
        "        # Inicialize os atributo self.epsilon, self.epsilon_decay e self.espilon_min\n",
        "        ...\n",
        "        ...\n",
        "        ...\n",
        "\n",
        "        # Inicialize o atributo de self.dqn com nosso objeto LinearNetwork(),\n",
        "        # utilize tamb√©m o m√©todo .to(self.device)\n",
        "        ...\n",
        "\n",
        "        # Inicialize o atributo self.optimizer com o optimizador optim.Adam() que optimiza o\n",
        "        # self.dqn.parameters() com um learning rate lr\n",
        "        ...\n",
        "\n",
        "    def act(self, state, greedy=False):\n",
        "        \"\"\"\n",
        "        M√©todo para o agente escolher uma a√ß√£o\n",
        "        \n",
        "        Par√¢metros\n",
        "        ----------\n",
        "        \n",
        "        state\n",
        "        O estado do agente\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        \n",
        "        action\n",
        "        A a√ß√£o escolhida pelo agente\n",
        "        \"\"\"\n",
        "\n",
        "        # Utilize do algoritmo epsilon-greedy:\n",
        "        # se um n√∫mero criado pela fun√ß√£o np.random.random() for menor\n",
        "        # que nosso epsilon e greedy for Falso, execute uma a√ß√£o aleat√≥ria\n",
        "        if ... :\n",
        "            ...\n",
        "        else:\n",
        "            with torch.no_grad(): # Utilizamos no_grad j√° que n√£o iremos optimzar esses par√¢metros agora\n",
        "                # Transforme o par√¢metro state em um torch.FloatTensor(), utilize o m√©todo .to(self.device)\n",
        "                ...\n",
        "                # Passe o state para nossa DQN pelo seu m√©todo forward e colete a a√ß√£o de maior valor com\n",
        "                # o m√©todo .argmax(dim=-1)\n",
        "                ...\n",
        "                # Transforme a a√ß√£o em um valor numpy com os m√©todos a√ß√£o.cpu().numpy()\n",
        "                ...\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def eps_decay(self):\n",
        "        \"\"\"\n",
        "        M√©todo para aplicar o decaimento do epsilon\n",
        "        \"\"\"\n",
        "\n",
        "        # Se self.epsilon for maior que o self.epsilon_min,\n",
        "        # self.epsilon = self.epsilon * self.epsilon_decay\n",
        "        ...\n",
        "        ...\n",
        "        \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        M√©todo para armazenar uma sequ√™ncia estado, a√ß√£o, recompensa, pr√≥ximo estado e done\n",
        "        no buffer de mem√≥ria\n",
        "        \"\"\"\n",
        "\n",
        "        # utilizando o m√©todo .update do nosso ReplaBuffer, passe os par√¢metros √† nossa mem√≥ria\n",
        "        ...\n",
        "    \n",
        "    def train(self, batch_size, save_file = False):\n",
        "        \"\"\" \n",
        "        M√©todo para treinar o agente\n",
        "        \"\"\"\n",
        "\n",
        "        # se batch_size * 10 for menor que o tamanho de nossa mem√≥ria, a a gente n√£o deve treinar\n",
        "        ...\n",
        "        ...\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Colete um batch de experi√™ncias com o m√©todo .sample da nossa mem√≥ria\n",
        "            ... \n",
        "\n",
        "            # Transforme cada atributo da nossa mem√≥ria em um tensor, para as actions,\n",
        "            # rewards, e dones utilize o m√©todo .unsqueeze(-1) para coloc√°-los no formato certo\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "\n",
        "            # Para obter nossos Q valores passe os states para nossa DQN pelo m√©todo .forward(),\n",
        "            # Utilize tamb√©m o m√©todo .gather(-1, actions.long()) ap√≥s o forward para obter os q valores\n",
        "            ...\n",
        "\n",
        "            with torch.no_grad():  # Utilizamos o no_grad pois esse q vamos usar para a loss, n√£o precisa dos gradientes\n",
        "                # Obtenha o Q2 passando os next_states para nossa DQN pelo m√©todo .forward(),\n",
        "                # utilize depois o m√©todo .max(dim=-1, keepdim=True)[0]\n",
        "                ...\n",
        "\n",
        "                # Calcule o target com a f√≥rmula (rewards + (1 - dones) * self.gamma * q2)\n",
        "                ...\n",
        "\n",
        "            # Calcule a loss\n",
        "            ...\n",
        "            # Realize as etapas de optimiza√ß√£o\n",
        "            ...\n",
        "            ...\n",
        "            ...\n",
        "\n",
        "        if save:\n",
        "            self.save_model()\n",
        "\n",
        "    def save_model(self, model_file):\n",
        "        torch.save(self.dqn.state_dict(), model_file)\n",
        "        print(f\"\\n Model saved: {model_file}\")\n",
        "\n",
        "    def load_model(self, model_file):\n",
        "        self.dqn.load_state_dict(torch.load(model_file))\n",
        "        print(f\"Model loaded: {model_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r27Og8jhomsR"
      },
      "source": [
        "### O que o m√©todo unsqueeze() faz?\n",
        "Basicamente ele insere uma dimens√£o de valor um em alguma dimens√£o especificada de um tensor, exemplos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_lClVfKomsW"
      },
      "source": [
        "a = torch.as_tensor([[1,2,3],[4,5,6]])\n",
        "print(f\"Tensor sem unsqueeze: \\n{a}\\n\"\n",
        "      f\"Formato do tensor sem unsqueeze {a.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpBozsK8omsW"
      },
      "source": [
        "b = a.unsqueeze(-1)\n",
        "print(f\"Tensor com unsqueeze: \\n{b}\\n\"\n",
        "      f\"Formato do tensor com unsqueeze {b.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErdS1AcwomsX"
      },
      "source": [
        "### O que o m√©todo gather() faz?\n",
        "Basicamente ele criar um tensor novo pegando os valores de um tensor (no nosso caso actions.long()) e os utilizando como √≠ndices para pegar valores do tensor de origem (sa√≠da de nossa rede neural). o par√¢metro dim √© utilziado para indicar por qual dimens√£o do tensor de √≠ndice deve ser percorrido.\n",
        "\n",
        "![img](https://i.stack.imgur.com/nudGq.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbhlrE8jomsX"
      },
      "source": [
        "## Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGouHqIomsX"
      },
      "source": [
        "def train(agent, env, timesteps, batch_size, render=False):\n",
        "    total_reward = 0\n",
        "    episode_returns = deque(maxlen=20)\n",
        "    avg_returns = []\n",
        "    episode = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for timestep in range(1, timesteps + 1):\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Tomar a a√ß√£o escolhida\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Guardar as informa√ß√µes geradas pela a√ß√£o\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        # Treinar a rede com base no ReplayBuffer\n",
        "        agent.train(batch_size, False)\n",
        "\n",
        "        # Soma as recompensas\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            episode_returns.append(total_reward)\n",
        "            episode += 1\n",
        "            next_state = env.reset()\n",
        "\n",
        "        agent.eps_decay()\n",
        "\n",
        "        if episode_returns:\n",
        "            avg_returns.append(np.mean(episode_returns))\n",
        "\n",
        "        total_reward *= 1 - done\n",
        "        ratio = math.ceil(100 * timestep / timesteps)\n",
        "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
        "\n",
        "        # Atualiza o estado\n",
        "        state = next_state\n",
        "\n",
        "        if render:\n",
        "            # Mostra o ambiente\n",
        "            env.render()\n",
        "\n",
        "        print(\n",
        "            f\"\\r[{ratio:3d}%]\",\n",
        "            f\"timestep = {timestep}/{timesteps}\",\n",
        "            f\"episode = {episode:3d}\",\n",
        "            f\"avg_return = {avg_return:10.4f}\",\n",
        "            f\"eps = {agent.epsilon:.4f}\",\n",
        "            sep=\", \",\n",
        "            end=\"\")\n",
        "\n",
        "    env.close()\n",
        "    return avg_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A3E3e96omsX"
      },
      "source": [
        "## Treinando o agente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZtLWGbSomsY"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.99\n",
        "EPS_INIT = 1\n",
        "EPS_END = 0.001\n",
        "EPS_DECAY = 0.99995\n",
        "MAX_MEMORY = 1_000_000\n",
        "TIMESTEPS = 150_000\n",
        "EPOCHS = 1\n",
        "\n",
        "env_name = 'LunarLander-v2'\n",
        "env = gym.make(env_name)\n",
        "OBS_SPACE = env.observation_space\n",
        "ACT_SPACE = env.action_space\n",
        "\n",
        "print(\"\\nTraining DQN\")\n",
        "dqn_net = DQNagent(observation_space=OBS_SPACE,\n",
        "                    action_space=ACT_SPACE,\n",
        "                    lr=3e-4,\n",
        "                    gamma=GAMMA,\n",
        "                    max_memory=MAX_MEMORY,\n",
        "                    epsilon_init=EPS_INIT,\n",
        "                    epsilon_decay=EPS_DECAY,\n",
        "                    epsilon_min=EPS_END,\n",
        "                    epochs=EPOCHS)\n",
        "\n",
        "\n",
        "results_dqn = train(dqn_net, env, TIMESTEPS, BATCH_SIZE, render=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOEdQpg3omsY"
      },
      "source": [
        "## Checando os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPYLNUWSomsY"
      },
      "source": [
        "# Plota os resultados\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(results_dqn)\n",
        "plt.title('DQN - m√©dia m√≥vel do retorno nos √∫timos 20 epis√≥dios')\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('retorno')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZloZmfomsZ"
      },
      "source": [
        "from IPython import display\n",
        "import base64\n",
        "\n",
        "def test(agent, env, episodes):\n",
        "    # O Monitor salva v√≠deos dos epis√≥dios na pasta \"monitor_dir\"\n",
        "    monitor_dir = 'monitor' + str(np.random.random())[2:]\n",
        "    env = gym.wrappers.Monitor(env, monitor_dir, video_callable=lambda ep: True)\n",
        "        \n",
        "    for episode in range(episodes):\n",
        "        done = False\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state, greedy=True)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            print(f\"\\r{total_reward: 7.3f}\", end=\"\")\n",
        "\n",
        "        print()\n",
        "    env.close()\n",
        "    \n",
        "    for video_file, stats_file in env.videos:\n",
        "        with open(video_file, 'rb') as f:\n",
        "            data_url = \"data:video/mp4;base64,\" + base64.b64encode(f.read()).decode()\n",
        "        display.display(display.HTML(f'<video controls><source src=\"{data_url}\" type=\"video/mp4\"></video>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kle-F8vJomsZ"
      },
      "source": [
        "env_name = 'LunarLander-v2'\n",
        "env = gym.make(env_name)\n",
        "test(dqn_net, env, 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
