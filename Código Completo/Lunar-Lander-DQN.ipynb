{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('RLLib': conda)",
   "metadata": {
    "interpreter": {
     "hash": "63eaf4b1ad596a07d93f6dd8c581cc9e56c653b50a3dbe88865e794f4676bc3f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# üöÄ Aula Aberta de DQN\n",
    "> Ensinando um m√≥dulo a pousar na Lua com redes neurais!\n",
    "\n",
    "> Voc√™ pode checar os slides da aula [aqui](#todo)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importando as bibliotecas necess√°rias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: Box2D in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (2.3.10)\n",
      "Requirement already satisfied: gym[box2d] in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (1.6.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (1.19.2)\n",
      "Requirement already satisfied: box2d-py~=2.3.5 in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from gym[box2d]) (2.3.8)\n",
      "Requirement already satisfied: future in /home/nelson/anaconda3/envs/RLLib/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install Box2D # Necess√°rio para nossos ambientes\n",
    "!pip install gym[box2d] # Ambientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "Voc√™ pode ver a implenta√ß√£o do replay buffer [aqui](./ReplayBuffer.py)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReplayBuffer import ReplayBuffer"
   ]
  },
  {
   "source": [
    "## Conhecendo nosso ambiente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Formato das observa√ß√µes do nosso agente: (8,) | Uma poss√≠vel observa√ß√£o: [ 0.8091425  -1.135604    2.359145   -0.7932247  -1.6031862   0.41732824\n -0.77799165 -0.60098314]\nN√∫mero de poss√≠veis a√ß√µes: 4 | Uma poss√≠vel a√ß√£o: 1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "print(f\"Formato das observa√ß√µes do nosso agente: {env.observation_space.shape} | Uma poss√≠vel observa√ß√£o: {env.observation_space.sample()}\")\n",
    "print(f\"N√∫mero de poss√≠veis a√ß√µes: {env.action_space.n} | Uma poss√≠vel a√ß√£o: {env.action_space.sample()}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "source": [
    "Como est√£o configuradas as recompensas:\n",
    "  - Se a nave pousar ela recebe uma recompensa de $+100$\n",
    "  - Cada perna que entra em contato com o solo o agente recebe $+10$\n",
    "  - Acionar as engines faz com que ele receba uma penalidade de $-0.3$ por frame\n",
    "  - O estado terminal ocorre quando ou o agente morre, recebendo uma penalidade de $-100$ ou quando acaba o tempo da simula√ß√£o, recebendo mais uma recompensa de $+100$ se estava pousado no alvo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Criando um agente aleat√≥rio\n",
    "S√≥ para ver se est√° tudo funcionando corretamente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_env():\n",
    "\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_env()"
   ]
  },
  {
   "source": [
    "## Rede Neural"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Cria uma rede neural para DQN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \"\"\"\n",
    "        Inicializa a rede\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        in_dim: int\n",
    "        Dimens√£o de entrada da rede, ou seja, o shape do estado do ambiente\n",
    "        \n",
    "        out_dim: int\n",
    "        N√∫mero de a√ß√µes do agente neste ambiente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        super(LinearNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Propaga uma entrada pela rede\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "source": [
    "## Criando nosso agente"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNagent:\n",
    "    \"\"\"\n",
    "    Uma classe que cria um agente DQN que utiliza ReplayBuffer como mem√≥ria\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 observation_space, \n",
    "                 action_space, \n",
    "                 lr=3e-4, \n",
    "                 gamma=0.99, \n",
    "                 max_memory=100000,\n",
    "                 epsilon_init=0.5,\n",
    "                 epsilon_decay=0.995,\n",
    "                 epsilon_min=0.01,\n",
    "                 epochs=1):\n",
    "      \n",
    "        \"\"\"\n",
    "        Inicializa o agente com os par√¢metros dados\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        \n",
    "        observation_space: gym.spaces\n",
    "        O espa√ßo de observa√ß√£o do gym\n",
    "         \n",
    "        action_space: gym.spaces\n",
    "        O espa√ßo de a√ß√µes do agente modelado no gym\n",
    "        \n",
    "        lr: floar, default=3e-4\n",
    "        A taxa de aprendizado do agente\n",
    "        \n",
    "        gamma: float, default=0.99\n",
    "        O fator de desconto. Se perto de 1. as recompensas futuras ter√£o grande import√¢ncia,\n",
    "        se perto de 0. as recompensas mais instant√¢neas ter√£o maior import√¢ncia\n",
    "        \n",
    "        max_memory: int, default=100000\n",
    "        O n√∫mero m√°ximo de transi√ß√µes armazenadas no buffer de mem√≥ria\n",
    "        \n",
    "        epsilon_init: float, default=0.5\n",
    "        O epsilon inicial do agente. Se pr√≥ximo de 1. o agente tomar√° muitas a√ß√µes\n",
    "        aleat√≥rias, se pro√≥ximo de 0. o agente escolher√° as a√ß√µes com maior\n",
    "        Q-valor\n",
    "        \n",
    "        epsilon_decay: float, default=0.9995\n",
    "        A taxa de decaimento do epsilon do agente. A cada treinamento o agente tende\n",
    "        a escolher meno a√ß√µes aleat√≥rias se epsilon_decay<1\n",
    "        \n",
    "        min_epsilon: float, default=0.01\n",
    "        O menor epsilon poss√≠vel\n",
    "        \n",
    "        \n",
    "        network: str, default='linear'\n",
    "        O tipo de rede a ser utilizada para o agente DQN. Por padr√£o √© usada uma rede linear, mas\n",
    "        pode ser usada uma rede convolucional se o par√¢metro for 'conv'\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using: {self.device}\")\n",
    "        self.gamma = gamma\n",
    "        self.memory = ReplayBuffer(max_memory, observation_space.shape[0])\n",
    "        self.action_space = action_space\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.epsilon = epsilon_init\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.dqn = LinearNetwork(observation_space.shape[0], action_space.n).to(self.device)\n",
    "        self.optmizer = optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        M√©todo para o agente escolher uma a√ß√£o\n",
    "        \n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        \n",
    "        state\n",
    "        O estado do agente\n",
    "        \n",
    "        Retorna\n",
    "        -------\n",
    "        \n",
    "        action\n",
    "        A a√ß√£o escolhida pelo agente\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                action = self.dqn.forward(state).argmax(dim=-1)\n",
    "                action = action.cpu().numpy()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def eps_decay(self):\n",
    "        \"\"\"\n",
    "        M√©todo para aplicar o decaimento do epsilon\n",
    "        \"\"\"\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        M√©todo para armazenar uma sequ√™ncia estado, a√ß√£o, recompensa, pr√≥ximo estado e done\n",
    "        no buffer de mem√≥ria\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory.update(state, action, rewards, next_state, done)\n",
    "\n",
    "    def train(self, batch_size, save=False):\n",
    "        \"\"\"\n",
    "        M√©todo para treinar o agente\n",
    "        \"\"\"\n",
    "        if batch_size * 10 > self.memory.size:\n",
    "            return\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            states, actions, rewards, next_states, dones = self.memory.sample(batch_size)\n",
    "\n",
    "            states = torch.as_tensor(states).to(self.device)\n",
    "            actions = torch.as_tensor(actions).to(self.device).unsqueeze(-1) # Unsqueeze adiciona uma dimens√£o \"1\" no √≠ndice indicado\n",
    "            rewards = torch.as_tensor(rewards).to(self.device).unsqueeze(-1) # no caso do -1, ele adiciona na √∫ltima dimens√£o\n",
    "            next_state = torch.as_tensor(next_states).to(self.device)\n",
    "            dones = torch.as_tensors(dones).to(self.device).unsqueeze(-1)\n",
    "\n",
    "            q = self.dqn.forward(states).gather(-1, actions.long()) # Pega-se os q-valores das a√ß√µes do batch\n",
    "\n",
    "            with torch.no_grad():   # Utilizamos o no_grad pois esse q vamos usar para a loss, n√£o precisa dos gradientes\n",
    "                q2 = self.dqn.forward(next_states).max(dim=-1, keepdim=True)[0]\n",
    "\n",
    "                target = (rewards + (1 - dones) * self.gamma * q2).to(self.device)\n",
    "\n",
    "            loss = F.mse_loss(q, target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "        if save:\n",
    "            self.save_model()\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        torch.save(self.dqn.state_dict(), model_file)\n",
    "        print(f\"\\n Model saved: {model_file}\")\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        self.dqn.load_state_dict(torch.load(model_file))\n",
    "        print(f\"Model loaded: {model_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, timesteps, batch_size, render=False):\n",
    "    total_reward = 0\n",
    "    episode_returns = deque(maxlen=20)\n",
    "    avg_returns = []\n",
    "    episode = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for timestep in range(1, timesteps + 1):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Tomar a a√ß√£o escolhida\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Guardar as informa√ß√µes geradas pela a√ß√£o\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Treinar a rede com base no ReplayBuffer\n",
    "        agent.train(batch_size, False)\n",
    "\n",
    "        # Soma as recompensas\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            episode_returns.append(total_reward)\n",
    "            episode += 1\n",
    "            next_state = env.reset()\n",
    "\n",
    "        agent.eps_decay()\n",
    "\n",
    "        if episode_returns:\n",
    "            avg_returns.append(np.mean(episode_returns))\n",
    "\n",
    "        total_reward *= 1 - done\n",
    "        ratio = math.ceil(100 * timestep / timesteps)\n",
    "        avg_return = avg_returns[-1] if avg_returns else np.nan\n",
    "\n",
    "        # Atualiza o estado\n",
    "        state = next_state\n",
    "\n",
    "        if render:\n",
    "            # Mostra o ambiente\n",
    "            env.render()\n",
    "\n",
    "        print(\n",
    "            f\"\\r[{ratio:3d}%]\",\n",
    "            f\"timestep = {timestep}/{timesteps}\",\n",
    "            f\"episode = {episode:3d}\",\n",
    "            f\"avg_return = {avg_return:10.4f}\",\n",
    "            f\"eps = {agent.epsilon:.4f}\",\n",
    "            sep=\", \",\n",
    "            end=\"\")\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "    return avg_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining DQN\nUsing: cpu\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'rewars' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-352854b8af56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mresults_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIMESTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d5e7f3af5710>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, timesteps, batch_size, render)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Guardar as informa√ß√µes geradas pela a√ß√£o\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Treinar a rede com base no ReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-f5035be6aa73>\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rewars' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.99\n",
    "EPS_INIT = 1\n",
    "EPS_END = 0.001\n",
    "EPS_DECAY = 0.99995\n",
    "MAX_MEMORY = 1_000_000\n",
    "TIMESTEPS = 150_000\n",
    "EPOCHS = 1\n",
    "\n",
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "OBS_SPACE = env.observation_space\n",
    "ACT_SPACE = env.action_space\n",
    "\n",
    "print(\"\\nTraining DQN\")\n",
    "dqn_net = DQNagent(observation_space=OBS_SPACE,\n",
    "                    action_space=ACT_SPACE,\n",
    "                    lr=3e-4,\n",
    "                    gamma=GAMMA,\n",
    "                    max_memory=MAX_MEMORY,\n",
    "                    epsilon_init=EPS_INIT,\n",
    "                    epsilon_decay=EPS_DECAY,\n",
    "                    epsilon_min=EPS_END,\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "\n",
    "results_dqn = train(dqn_net, env, TIMESTEPS, BATCH_SIZE, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}